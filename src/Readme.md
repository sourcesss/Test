#思路：
1. 每个批次的文件都很大，因此需要将这个文件拆分成多个小文件，重新生成多个新的小文件，
   每个小文件大小都是内存可映射的大小 <br/>
   为了方便统计，拆分的方法如下，把每条记录对应的url取hash值，然后跟文件数据量取模，
   放入相应的文件中，这样，相同的url的记录都出现在相同的文件中了
    
2. 为了防止文件被重复处理，每次处理文件时都生成一个同样文件名,后缀为.done的文件，
   这样在扫描路径时如果一个文件有对应的.done 文件就跳过处理
   
3. 针对拆分好的小文件可以通过增加机器的数量，来提高文件处理的效率

4. 针对单个小文件而言，在内存中做数据统计，统计出每个url对应的访问总数， 及前20个用户，
再计算出前20个和后20个访问量的url
   
5. 最后再把每个文件的统计结果做合并，就能统计访问量出，想要的结果
